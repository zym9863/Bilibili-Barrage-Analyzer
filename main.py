"""
Bilibiliå¼¹å¹•åˆ†æå™¨ - Streamlit Webåº”ç”¨
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, date
import time
import traceback
from config import config

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from danmaku_fetcher import DanmakuFetcher, fetch_danmaku, fetch_video_info
from danmaku_analyzer import DanmakuAnalyzer, analyze_danmaku
from danmaku_visualizer import DanmakuVisualizer, create_visualizations
from danmaku_ai_analyzer import DanmakuAIAnalyzer, analyze_danmaku_with_ai
from utils import progress_callback, data_cache
from validator import validator


def main():
    """ä¸»å‡½æ•°"""
    # é¡µé¢é…ç½®
    st.set_page_config(
        page_title="Bilibiliå¼¹å¹•åˆ†æå™¨",
        page_icon="ğŸ“º",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # æ ‡é¢˜å’Œæè¿°
    st.title("ğŸ“º Bilibiliå¼¹å¹•åˆ†æå™¨")
    st.markdown("---")
    st.markdown("### ğŸ¯ åŠŸèƒ½ä»‹ç»")
    st.markdown("""
    - ğŸ“¥ **å¼¹å¹•è·å–**: æ”¯æŒä»Bilibiliè§†é¢‘è·å–å¼¹å¹•æ•°æ®
    - ğŸ“Š **æ•°æ®åˆ†æ**: è¯é¢‘ç»Ÿè®¡ã€æƒ…æ„Ÿåˆ†æã€æ—¶é—´åˆ†å¸ƒåˆ†æ
    - ğŸ“ˆ **å¯è§†åŒ–**: è¯äº‘å›¾ã€æ—¶é—´åˆ†å¸ƒå›¾ã€æƒ…æ„Ÿåˆ†æå›¾ç­‰
    - ğŸ” **çƒ­ç‚¹å‘ç°**: è‡ªåŠ¨è¯†åˆ«å¼¹å¹•çƒ­ç‚¹æ—¶åˆ»
    - ğŸ¤– **AIæ™ºèƒ½åˆ†æ**: ä½¿ç”¨AIæ·±åº¦åˆ†æå¼¹å¹•å†…å®¹å’Œè§‚ä¼—ååº”
    """)

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")

        # è§†é¢‘URLè¾“å…¥
        st.subheader("ğŸ“º è§†é¢‘ä¿¡æ¯")
        video_input = st.text_input(
            "è¯·è¾“å…¥Bilibiliè§†é¢‘URLæˆ–BVå·:",
            placeholder="ä¾‹å¦‚: https://www.bilibili.com/video/BV1P24y1a7Lt æˆ– BV1P24y1a7Lt",
            help="æ”¯æŒå®Œæ•´URLæˆ–ç›´æ¥è¾“å…¥BVå·"
        )

        # åˆ†æé€‰é¡¹
        st.subheader("ğŸ”§ åˆ†æé€‰é¡¹")
        use_protobuf = st.checkbox("ä½¿ç”¨Protobufæ¥å£", value=True, help="æ¨èä½¿ç”¨ï¼Œæ•°æ®æ›´å®Œæ•´")
        page_number = st.number_input("åˆ†På·", min_value=0, value=0, help="å¤šPè§†é¢‘çš„åˆ†é›†å·ï¼Œä»0å¼€å§‹")

        # AIåˆ†æé€‰é¡¹
        st.subheader("ğŸ¤– AIåˆ†æ")
        enable_ai_analysis = st.checkbox("å¯ç”¨AIæ™ºèƒ½åˆ†æ", value=False, help="ä½¿ç”¨AIè¿›è¡Œæ·±åº¦å†…å®¹åˆ†æï¼ˆéœ€è¦ç½‘ç»œè¿æ¥ï¼‰")
        
        if enable_ai_analysis:
            st.info("ğŸ” AIåˆ†æå°†æä¾›æƒ…æ„Ÿåˆ†æã€ä¸»é¢˜è¯†åˆ«ã€çƒ­ç‚¹è§£è¯»å’Œç»¼åˆæŠ¥å‘Š")

        # é«˜çº§é€‰é¡¹
        with st.expander("ğŸ”¬ é«˜çº§é€‰é¡¹"):
            date_filter = st.date_input(
                "æ—¥æœŸè¿‡æ»¤ (å¯é€‰)",
                value=None,
                help="åªè·å–æŒ‡å®šæ—¥æœŸçš„å¼¹å¹•ï¼Œç•™ç©ºè¡¨ç¤ºè·å–æ‰€æœ‰"
            )

            ui_settings = config.ui_settings
            keyword_count = st.slider(
                "å…³é”®è¯æ•°é‡", 
                ui_settings['keyword_count_min'], 
                ui_settings['keyword_count_max'], 
                ui_settings['keyword_count_default'], 
                help="æ˜¾ç¤ºçš„çƒ­é—¨å…³é”®è¯æ•°é‡"
            )
            time_interval = st.slider(
                "æ—¶é—´é—´éš” (ç§’)", 
                ui_settings['time_interval_min'], 
                ui_settings['time_interval_max'], 
                ui_settings['time_interval_default'], 
                help="æ—¶é—´åˆ†å¸ƒåˆ†æçš„æ—¶é—´é—´éš”"
            )

    # ä¸»å†…å®¹åŒºåŸŸ
    if video_input:
        # éªŒè¯è¾“å…¥
        try:
            validated_input = validator.validate_bilibili_url(video_input)
        except Exception as e:
            st.error(f"âŒ URLæ ¼å¼é”™è¯¯: {validator.sanitize_error_message(str(e))}")
            return
            
        # è·å–è§†é¢‘ä¿¡æ¯
        with st.spinner("æ­£åœ¨è·å–è§†é¢‘ä¿¡æ¯..."):
            try:
                video_info = fetch_video_info(video_input)

                if video_info:
                    # æ˜¾ç¤ºè§†é¢‘ä¿¡æ¯
                    col1, col2, col3 = st.columns([2, 1, 1])

                    with col1:
                        st.subheader("ğŸ“¹ è§†é¢‘ä¿¡æ¯")
                        st.write(f"**æ ‡é¢˜**: {video_info.get('title', 'æœªçŸ¥')}")
                        st.write(f"**æ—¶é•¿**: {format_duration(video_info.get('duration', 0))}")
                        if video_info.get('pages', 1) > 1:
                            st.write(f"**åˆ†Pæ•°**: {video_info.get('pages', 1)}")

                    with col2:
                        st.metric("ğŸ‘€ æ’­æ”¾é‡", format_number(video_info.get('view', 0)))
                        st.metric("ğŸ’¬ å¼¹å¹•æ•°", format_number(video_info.get('danmaku', 0)))

                    with col3:
                        st.metric("ğŸ‘ ç‚¹èµ", format_number(video_info.get('like', 0)))
                        st.metric("â­ æ”¶è—", format_number(video_info.get('favorite', 0)))

                    st.markdown("---")

                    # å¼€å§‹åˆ†ææŒ‰é’®
                    if st.button("ğŸš€ å¼€å§‹åˆ†æå¼¹å¹•", type="primary", use_container_width=True):
                        analyze_danmaku_data(video_input, page_number, use_protobuf, date_filter, keyword_count, time_interval, enable_ai_analysis)

                else:
                    st.error("âŒ æ— æ³•è·å–è§†é¢‘ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥URLæˆ–BVå·æ˜¯å¦æ­£ç¡®")

            except Exception as e:
                st.error(f"âŒ è·å–è§†é¢‘ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
                st.error("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")

    else:
        # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
        st.info("ğŸ‘† è¯·åœ¨å·¦ä¾§è¾“å…¥Bilibiliè§†é¢‘URLæˆ–BVå·å¼€å§‹åˆ†æ")

        # ç¤ºä¾‹å±•ç¤º
        st.subheader("ğŸ“– ä½¿ç”¨è¯´æ˜")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **æ”¯æŒçš„è¾“å…¥æ ¼å¼:**
            - å®Œæ•´URL: `https://www.bilibili.com/video/BV1P24y1a7Lt`
            - çŸ­é“¾æ¥: `https://b23.tv/xxxxx`
            - BVå·: `BV1P24y1a7Lt`
            """)

        with col2:
            st.markdown("""
            **åˆ†æåŠŸèƒ½:**
            - ğŸ”¤ å…³é”®è¯æå–å’Œè¯é¢‘ç»Ÿè®¡
            - ğŸ˜Š æƒ…æ„Ÿå€¾å‘åˆ†æ
            - â° å¼¹å¹•æ—¶é—´åˆ†å¸ƒ
            - ğŸ”¥ çƒ­ç‚¹æ—¶åˆ»è¯†åˆ«
            - ğŸ¤– AIæ™ºèƒ½å†…å®¹åˆ†æ
            """)


def analyze_danmaku_data(video_input, page_number, use_protobuf, date_filter, keyword_count, time_interval, enable_ai_analysis=False):
    """åˆ†æå¼¹å¹•æ•°æ®"""

    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()

    try:
        # æ­¥éª¤1: è·å–å¼¹å¹•æ•°æ®
        status_text.text("ğŸ“¥ æ­£åœ¨è·å–å¼¹å¹•æ•°æ®...")
        progress_bar.progress(20)

        danmaku_data = fetch_danmaku(
            video_input,
            page=page_number,
            use_protobuf=use_protobuf,
            date_filter=date_filter
        )

        if not danmaku_data:
            st.error("âŒ æœªè·å–åˆ°å¼¹å¹•æ•°æ®ï¼Œå¯èƒ½æ˜¯è§†é¢‘æ²¡æœ‰å¼¹å¹•æˆ–ç½‘ç»œé—®é¢˜")
            return

        progress_bar.progress(40)

        # æ­¥éª¤2: åˆ†ææ•°æ®
        status_text.text("ğŸ” æ­£åœ¨åˆ†æå¼¹å¹•æ•°æ®...")

        analyzer = DanmakuAnalyzer()
        analysis_result = analyzer.generate_summary_report(danmaku_data, time_interval)

        progress_bar.progress(60)

        # æ­¥éª¤3: AIåˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        ai_results = None
        if enable_ai_analysis:
            status_text.text("ğŸ¤– æ­£åœ¨è¿›è¡ŒAIæ™ºèƒ½åˆ†æ...")
            try:
                danmaku_texts = [item['text'] for item in danmaku_data if item.get('text')]
                ai_results = analyze_danmaku_with_ai(analysis_result, danmaku_texts)
                progress_bar.progress(80)
            except Exception as e:
                st.warning(f"âš ï¸ AIåˆ†æå¤±è´¥: {str(e)}")
                ai_results = None
                progress_bar.progress(80)
        else:
            progress_bar.progress(80)

        # æ­¥éª¤4: ç”Ÿæˆå¯è§†åŒ–
        status_text.text("ğŸ“Š æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        visualizer = DanmakuVisualizer()
        figures = visualizer.create_dashboard(analysis_result)

        progress_bar.progress(100)
        status_text.text("âœ… åˆ†æå®Œæˆ!")

        time.sleep(1)
        progress_bar.empty()
        status_text.empty()

        # æ˜¾ç¤ºåˆ†æç»“æœ
        display_analysis_results(analysis_result, figures, danmaku_data, ai_results)

    except Exception as e:
        progress_bar.empty()
        status_text.empty()
        st.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        st.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        st.code(traceback.format_exc())


def display_analysis_results(analysis_result, figures, danmaku_data, ai_results=None):
    """æ˜¾ç¤ºåˆ†æç»“æœ"""

    st.markdown("---")
    st.header("ğŸ“Š åˆ†æç»“æœ")

    # åŸºæœ¬ç»Ÿè®¡
    if 'basic_stats' in analysis_result:
        st.subheader("ğŸ“ˆ åŸºæœ¬ç»Ÿè®¡")

        col1, col2, col3, col4 = st.columns(4)

        stats = analysis_result['basic_stats']

        with col1:
            st.metric("æ€»å¼¹å¹•æ•°", stats.get('total_count', 0))

        with col2:
            st.metric("ç‹¬ç‰¹å¼¹å¹•æ•°", stats.get('unique_count', 0))

        with col3:
            duplicate_rate = stats.get('duplicate_rate', 0)
            st.metric("é‡å¤ç‡", f"{duplicate_rate:.1%}")

        with col4:
            avg_length = analysis_result.get('length_stats', {}).get('mean_length', 0)
            st.metric("å¹³å‡é•¿åº¦", f"{avg_length:.1f}å­—")

    # AIåˆ†æç»“æœ
    if ai_results:
        st.markdown("---")
        st.subheader("ğŸ¤– AIæ™ºèƒ½åˆ†æ")
        
        # åˆ›å»ºAIåˆ†ææ ‡ç­¾é¡µ
        ai_tab1, ai_tab2, ai_tab3, ai_tab4 = st.tabs(["ğŸ˜Š AIæƒ…æ„Ÿåˆ†æ", "ğŸ¯ ä¸»é¢˜åˆ†æ", "ğŸ”¥ çƒ­ç‚¹è§£è¯»", "ğŸ“‹ ç»¼åˆæŠ¥å‘Š"])
        
        with ai_tab1:
            if 'sentiment_ai' in ai_results and 'analysis' in ai_results['sentiment_ai']:
                st.write("**AIæƒ…æ„Ÿåˆ†æç»“æœ:**")
                st.write(ai_results['sentiment_ai']['analysis'])
                if 'sample_count' in ai_results['sentiment_ai']:
                    st.caption(f"åŸºäº {ai_results['sentiment_ai']['sample_count']} æ¡å¼¹å¹•æ ·æœ¬åˆ†æ")
            elif 'sentiment_ai' in ai_results and 'error' in ai_results['sentiment_ai']:
                st.error(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {ai_results['sentiment_ai']['error']}")
            else:
                st.info("æš‚æ— AIæƒ…æ„Ÿåˆ†ææ•°æ®")
        
        with ai_tab2:
            if 'themes_ai' in ai_results and 'analysis' in ai_results['themes_ai']:
                st.write("**AIä¸»é¢˜åˆ†æç»“æœ:**")
                st.write(ai_results['themes_ai']['analysis'])
                if 'sample_count' in ai_results['themes_ai']:
                    st.caption(f"åŸºäº {ai_results['themes_ai']['sample_count']} æ¡å¼¹å¹•æ ·æœ¬åˆ†æ")
            elif 'themes_ai' in ai_results and 'error' in ai_results['themes_ai']:
                st.error(f"ä¸»é¢˜åˆ†æå¤±è´¥: {ai_results['themes_ai']['error']}")
            else:
                st.info("æš‚æ— AIä¸»é¢˜åˆ†ææ•°æ®")
        
        with ai_tab3:
            if 'hot_moments_ai' in ai_results and 'analysis' in ai_results['hot_moments_ai']:
                st.write("**AIçƒ­ç‚¹æ—¶åˆ»åˆ†æ:**")
                st.write(ai_results['hot_moments_ai']['analysis'])
                if 'hot_moments_count' in ai_results['hot_moments_ai']:
                    st.caption(f"åŸºäº {ai_results['hot_moments_ai']['hot_moments_count']} ä¸ªçƒ­ç‚¹æ—¶åˆ»åˆ†æ")
            elif 'hot_moments_ai' in ai_results and 'error' in ai_results['hot_moments_ai']:
                st.error(f"çƒ­ç‚¹åˆ†æå¤±è´¥: {ai_results['hot_moments_ai']['error']}")
            else:
                st.info("æš‚æ— AIçƒ­ç‚¹åˆ†ææ•°æ®")
        
        with ai_tab4:
            if 'comprehensive_ai' in ai_results and 'comprehensive_report' in ai_results['comprehensive_ai']:
                st.write("**AIç»¼åˆåˆ†ææŠ¥å‘Š:**")
                st.write(ai_results['comprehensive_ai']['comprehensive_report'])
                if 'sample_count' in ai_results['comprehensive_ai']:
                    st.caption(f"åŸºäº {ai_results['comprehensive_ai']['sample_count']} æ¡å¼¹å¹•æ ·æœ¬åˆ†æ")
            elif 'comprehensive_ai' in ai_results and 'error' in ai_results['comprehensive_ai']:
                st.error(f"ç»¼åˆåˆ†æå¤±è´¥: {ai_results['comprehensive_ai']['error']}")
            else:
                st.info("æš‚æ— AIç»¼åˆåˆ†ææ•°æ®")

    # å¯è§†åŒ–å›¾è¡¨
    if figures:
        st.markdown("---")
        st.subheader("ğŸ“Š æ•°æ®å¯è§†åŒ–")

        # åˆ›å»ºæ ‡ç­¾é¡µ
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["â˜ï¸ è¯äº‘å›¾", "ğŸ˜Š æƒ…æ„Ÿåˆ†æ", "ğŸ”¤ çƒ­é—¨å…³é”®è¯", "â° æ—¶é—´åˆ†å¸ƒ", "ğŸ“ é•¿åº¦åˆ†å¸ƒ", "ğŸ”¥ çƒ­ç‚¹æ—¶åˆ»"])

        with tab1:
            if 'wordcloud' in figures:
                st.plotly_chart(figures['wordcloud'], use_container_width=True)
            else:
                st.info("æš‚æ— è¯äº‘æ•°æ®")

        with tab2:
            if 'sentiment' in figures:
                st.plotly_chart(figures['sentiment'], use_container_width=True)
            else:
                st.info("æš‚æ— æƒ…æ„Ÿåˆ†ææ•°æ®")

        with tab3:
            if 'keywords' in figures:
                st.plotly_chart(figures['keywords'], use_container_width=True)
            else:
                st.info("æš‚æ— å…³é”®è¯æ•°æ®")

        with tab4:
            if 'time_distribution' in figures:
                st.plotly_chart(figures['time_distribution'], use_container_width=True)
            else:
                st.info("æš‚æ— æ—¶é—´åˆ†å¸ƒæ•°æ®")

        with tab5:
            if 'length_distribution' in figures:
                st.plotly_chart(figures['length_distribution'], use_container_width=True)
            else:
                st.info("æš‚æ— é•¿åº¦åˆ†å¸ƒæ•°æ®")

        with tab6:
            if 'hot_moments' in figures:
                st.plotly_chart(figures['hot_moments'], use_container_width=True)
            else:
                st.info("æš‚æ— çƒ­ç‚¹æ—¶åˆ»æ•°æ®")

    # è¯¦ç»†æ•°æ®
    with st.expander("ğŸ“‹ è¯¦ç»†æ•°æ®"):

        # çƒ­ç‚¹æ—¶åˆ»è¯¦æƒ…
        if 'hot_moments' in analysis_result and analysis_result['hot_moments']:
            st.subheader("ğŸ”¥ çƒ­ç‚¹æ—¶åˆ»è¯¦æƒ…")

            for i, moment in enumerate(analysis_result['hot_moments'][:5], 1):
                with st.container():
                    col1, col2 = st.columns([1, 3])

                    with col1:
                        st.write(f"**#{i}**")
                        st.write(f"æ—¶é—´: {format_duration(moment['time_start'])}")
                        st.write(f"å¼¹å¹•æ•°: {moment['count']}")

                    with col2:
                        st.write("**æ ·æœ¬å¼¹å¹•:**")
                        for j, text in enumerate(moment['sample_danmaku'][:3], 1):
                            st.write(f"{j}. {text}")

                    st.markdown("---")

        # åŸå§‹æ•°æ®ä¸‹è½½
        if danmaku_data:
            st.subheader("ğŸ’¾ æ•°æ®ä¸‹è½½")

            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(danmaku_data)

            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            st.write("**æ•°æ®é¢„è§ˆ:**")
            st.dataframe(df.head(10), use_container_width=True)

            # ä¸‹è½½æŒ‰é’®
            csv = df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½å®Œæ•´å¼¹å¹•æ•°æ® (CSV)",
                data=csv,
                file_name=f"danmaku_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )


def format_duration(seconds):
    """æ ¼å¼åŒ–æ—¶é•¿æ˜¾ç¤º"""
    if seconds < 60:
        return f"{seconds}ç§’"
    elif seconds < 3600:
        minutes = seconds // 60
        secs = seconds % 60
        return f"{minutes}åˆ†{secs}ç§’"
    else:
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        return f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ"


def format_number(num):
    """æ ¼å¼åŒ–æ•°å­—æ˜¾ç¤º"""
    if num >= 100000000:  # 1äº¿
        return f"{num/100000000:.1f}äº¿"
    elif num >= 10000:  # 1ä¸‡
        return f"{num/10000:.1f}ä¸‡"
    else:
        return str(num)


if __name__ == "__main__":
    main()
