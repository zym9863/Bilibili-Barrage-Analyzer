"""
Bilibiliå¼¹å¹•åˆ†æå™¨ - Streamlit Webåº”ç”¨
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, date
import time
import traceback
from config import config

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from danmaku_fetcher import DanmakuFetcher, fetch_danmaku, fetch_video_info
from danmaku_analyzer import DanmakuAnalyzer, analyze_danmaku
from danmaku_visualizer import DanmakuVisualizer, create_visualizations
from danmaku_ai_analyzer import DanmakuAIAnalyzer, analyze_danmaku_with_ai
from utils import progress_callback, data_cache
from validator import validator


def main():
    """ä¸»å‡½æ•°"""
    # é¡µé¢é…ç½®
    st.set_page_config(
        page_title="Bilibiliå¼¹å¹•åˆ†æå™¨",
        page_icon="ğŸ“º",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # æ ‡é¢˜å’Œæè¿°
    st.title("ğŸ“º Bilibiliå¼¹å¹•åˆ†æå™¨")
    st.markdown("---")
    st.markdown("### ğŸ¯ åŠŸèƒ½ä»‹ç»")
    st.markdown("""
    - ğŸ“¥ **å¼¹å¹•è·å–**: æ”¯æŒä»Bilibiliè§†é¢‘è·å–å¼¹å¹•æ•°æ®
    - ğŸ“Š **æ•°æ®åˆ†æ**: è¯é¢‘ç»Ÿè®¡ã€æƒ…æ„Ÿåˆ†æã€æ—¶é—´åˆ†å¸ƒåˆ†æ
    - ğŸ“ˆ **å¯è§†åŒ–**: è¯äº‘å›¾ã€æ—¶é—´åˆ†å¸ƒå›¾ã€æƒ…æ„Ÿåˆ†æå›¾ç­‰
    - ğŸ” **çƒ­ç‚¹å‘ç°**: è‡ªåŠ¨è¯†åˆ«å¼¹å¹•çƒ­ç‚¹æ—¶åˆ»
    - ğŸ¤– **AIæ™ºèƒ½åˆ†æ**: ä½¿ç”¨AIæ·±åº¦åˆ†æå¼¹å¹•å†…å®¹å’Œè§‚ä¼—ååº”
    """)

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")

        # è§†é¢‘URLè¾“å…¥
        st.subheader("ğŸ“º è§†é¢‘ä¿¡æ¯")
        video_input = st.text_input(
            "è¯·è¾“å…¥Bilibiliè§†é¢‘URLæˆ–BVå·:",
            placeholder="ä¾‹å¦‚: https://www.bilibili.com/video/BV1P24y1a7Lt æˆ– BV1P24y1a7Lt",
            help="æ”¯æŒå®Œæ•´URLæˆ–ç›´æ¥è¾“å…¥BVå·"
        )

        # åˆ†æé€‰é¡¹
        st.subheader("ğŸ”§ åˆ†æé€‰é¡¹")
        use_protobuf = st.checkbox("ä½¿ç”¨Protobufæ¥å£", value=True, help="æ¨èä½¿ç”¨ï¼Œæ•°æ®æ›´å®Œæ•´")
        page_number = st.number_input("åˆ†På·", min_value=0, value=0, help="å¤šPè§†é¢‘çš„åˆ†é›†å·ï¼Œä»0å¼€å§‹")

        # AIåˆ†æé€‰é¡¹
        st.subheader("ğŸ¤– AIåˆ†æ")
        enable_ai_analysis = st.checkbox("å¯ç”¨AIæ™ºèƒ½åˆ†æ", value=False, help="ä½¿ç”¨AIè¿›è¡Œæ·±åº¦å†…å®¹åˆ†æï¼ˆéœ€è¦ç½‘ç»œè¿æ¥ï¼‰")
        
        if enable_ai_analysis:
            st.info("ğŸ” AIåˆ†æå°†æä¾›æƒ…æ„Ÿåˆ†æã€ä¸»é¢˜è¯†åˆ«ã€çƒ­ç‚¹è§£è¯»å’Œç»¼åˆæŠ¥å‘Š")

        # é«˜çº§é€‰é¡¹
        with st.expander("ğŸ”¬ é«˜çº§é€‰é¡¹"):
            date_filter = st.date_input(
                "æ—¥æœŸè¿‡æ»¤ (å¯é€‰)",
                value=None,
                help="åªè·å–æŒ‡å®šæ—¥æœŸçš„å¼¹å¹•ï¼Œç•™ç©ºè¡¨ç¤ºè·å–æ‰€æœ‰"
            )

            ui_settings = config.ui_settings
            keyword_count = st.slider(
                "å…³é”®è¯æ•°é‡", 
                ui_settings['keyword_count_min'], 
                ui_settings['keyword_count_max'], 
                ui_settings['keyword_count_default'], 
                help="æ˜¾ç¤ºçš„çƒ­é—¨å…³é”®è¯æ•°é‡"
            )
            time_interval = st.slider(
                "æ—¶é—´é—´éš” (ç§’)", 
                ui_settings['time_interval_min'], 
                ui_settings['time_interval_max'], 
                ui_settings['time_interval_default'], 
                help="æ—¶é—´åˆ†å¸ƒåˆ†æçš„æ—¶é—´é—´éš”"
            )

    # ä¸»å†…å®¹åŒºåŸŸ
    if video_input:
        # éªŒè¯è¾“å…¥
        try:
            validated_input = validator.validate_bilibili_url(video_input)
            # éªŒè¯åˆ†På·
            validated_page = validator.validate_page_number(page_number)
            # éªŒè¯åˆ†æå‚æ•°
            validated_time_interval, validated_keyword_count = validator.validate_analysis_parameters(time_interval, keyword_count)
        except (ValueError, TypeError) as e:
            st.error(f"âŒ è¾“å…¥éªŒè¯å¤±è´¥: {validator.sanitize_error_message(str(e))}")
            return
            
        # è·å–è§†é¢‘ä¿¡æ¯
        with st.spinner("æ­£åœ¨è·å–è§†é¢‘ä¿¡æ¯..."):
            try:
                video_info = fetch_video_info(video_input)

                if video_info:
                    # æ˜¾ç¤ºè§†é¢‘ä¿¡æ¯
                    ui_config = config.ui_display_settings
                    video_columns = ui_config.get('video_info_columns', [2, 1, 1])
                    col1, col2, col3 = st.columns(video_columns)

                    with col1:
                        st.subheader("ğŸ“¹ è§†é¢‘ä¿¡æ¯")
                        st.write(f"**æ ‡é¢˜**: {video_info.get('title', 'æœªçŸ¥')}")
                        st.write(f"**æ—¶é•¿**: {format_duration(video_info.get('duration', 0))}")
                        if video_info.get('pages', 1) > 1:
                            st.write(f"**åˆ†Pæ•°**: {video_info.get('pages', 1)}")

                    with col2:
                        st.metric("ğŸ‘€ æ’­æ”¾é‡", format_number(video_info.get('view', 0)))
                        st.metric("ğŸ’¬ å¼¹å¹•æ•°", format_number(video_info.get('danmaku', 0)))

                    with col3:
                        st.metric("ğŸ‘ ç‚¹èµ", format_number(video_info.get('like', 0)))
                        st.metric("â­ æ”¶è—", format_number(video_info.get('favorite', 0)))

                    st.markdown("---")

                    # å¼€å§‹åˆ†ææŒ‰é’®
                    if st.button("ğŸš€ å¼€å§‹åˆ†æå¼¹å¹•", type="primary", use_container_width=True):
                        analyze_danmaku_data(
                            validated_input['url'], 
                            validated_page, 
                            use_protobuf, 
                            date_filter, 
                            validated_keyword_count, 
                            validated_time_interval, 
                            enable_ai_analysis
                        )

                else:
                    st.error("âŒ æ— æ³•è·å–è§†é¢‘ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥URLæˆ–BVå·æ˜¯å¦æ­£ç¡®")

            except (ConnectionError, TimeoutError) as e:
                st.error(f"âŒ ç½‘ç»œè¿æ¥é—®é¢˜: {str(e)}")
                st.error("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
            except (ValueError, KeyError) as e:
                st.error(f"âŒ æ•°æ®è§£æé”™è¯¯: {str(e)}")
                st.error("è¯·æ£€æŸ¥URLæˆ–BVå·æ˜¯å¦æ­£ç¡®")
            except Exception as e:
                st.error(f"âŒ è·å–è§†é¢‘ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
                st.error("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")

    else:
        # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
        st.info("ğŸ‘† è¯·åœ¨å·¦ä¾§è¾“å…¥Bilibiliè§†é¢‘URLæˆ–BVå·å¼€å§‹åˆ†æ")

        # ç¤ºä¾‹å±•ç¤º
        st.subheader("ğŸ“– ä½¿ç”¨è¯´æ˜")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **æ”¯æŒçš„è¾“å…¥æ ¼å¼:**
            - å®Œæ•´URL: `https://www.bilibili.com/video/BV1P24y1a7Lt`
            - çŸ­é“¾æ¥: `https://b23.tv/xxxxx`
            - BVå·: `BV1P24y1a7Lt`
            """)

        with col2:
            st.markdown("""
            **åˆ†æåŠŸèƒ½:**
            - ğŸ”¤ å…³é”®è¯æå–å’Œè¯é¢‘ç»Ÿè®¡
            - ğŸ˜Š æƒ…æ„Ÿå€¾å‘åˆ†æ
            - â° å¼¹å¹•æ—¶é—´åˆ†å¸ƒ
            - ğŸ”¥ çƒ­ç‚¹æ—¶åˆ»è¯†åˆ«
            - ğŸ¤– AIæ™ºèƒ½å†…å®¹åˆ†æ
            """)


def fetch_danmaku_data_step(video_input, page_number, use_protobuf, date_filter):
    """
    æ­¥éª¤1: è·å–å¼¹å¹•æ•°æ®
    
    Args:
        video_input: è§†é¢‘è¾“å…¥
        page_number: åˆ†På·
        use_protobuf: æ˜¯å¦ä½¿ç”¨protobuf
        date_filter: æ—¥æœŸè¿‡æ»¤å™¨
        
    Returns:
        List[Dict]: å¼¹å¹•æ•°æ®åˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    try:
        danmaku_data = fetch_danmaku(
            video_input,
            page=page_number,
            use_protobuf=use_protobuf,
            date_filter=date_filter
        )
        
        if not danmaku_data:
            st.error("âŒ æœªè·å–åˆ°å¼¹å¹•æ•°æ®ï¼Œå¯èƒ½æ˜¯è§†é¢‘æ²¡æœ‰å¼¹å¹•æˆ–ç½‘ç»œé—®é¢˜")
            return None
            
        return danmaku_data
        
    except Exception as e:
        st.error(f"âŒ è·å–å¼¹å¹•æ•°æ®å¤±è´¥: {str(e)}")
        return None


def analyze_danmaku_step(danmaku_data, time_interval):
    """
    æ­¥éª¤2: åˆ†æå¼¹å¹•æ•°æ®
    
    Args:
        danmaku_data: å¼¹å¹•æ•°æ®
        time_interval: æ—¶é—´é—´éš”
        
    Returns:
        åˆ†æç»“æœå­—å…¸ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    try:
        analyzer = DanmakuAnalyzer()
        analysis_result = analyzer.generate_summary_report(danmaku_data, time_interval)
        return analysis_result
        
    except Exception as e:
        st.error(f"âŒ å¼¹å¹•æ•°æ®åˆ†æå¤±è´¥: {str(e)}")
        return None


def perform_ai_analysis_step(analysis_result, danmaku_data, enable_ai_analysis):
    """
    æ­¥éª¤3: AIæ™ºèƒ½åˆ†æ
    
    Args:
        analysis_result: åŸºç¡€åˆ†æç»“æœ
        danmaku_data: å¼¹å¹•æ•°æ®
        enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æ
        
    Returns:
        AIåˆ†æç»“æœï¼Œå¦‚æœæœªå¯ç”¨æˆ–å¤±è´¥åˆ™è¿”å›None
    """
    if not enable_ai_analysis:
        return None
        
    try:
        danmaku_texts = [item['text'] for item in danmaku_data if item.get('text')]
        ai_results = analyze_danmaku_with_ai(analysis_result, danmaku_texts)
        return ai_results
        
    except (ConnectionError, TimeoutError) as e:
        st.warning("âš ï¸ AIåˆ†æç½‘ç»œè¶…æ—¶: ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•")
        return None
    except (ValueError, KeyError) as e:
        st.warning("âš ï¸ AIåˆ†ææ•°æ®é”™è¯¯: æ•°æ®æ ¼å¼é—®é¢˜")
        return None
    except Exception as e:
        st.warning(f"âš ï¸ AIåˆ†æå¤±è´¥: {str(e)}")
        return None


def generate_visualization_step(analysis_result):
    """
    æ­¥éª¤4: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    
    Args:
        analysis_result: åˆ†æç»“æœ
        
    Returns:
        å¯è§†åŒ–å›¾è¡¨å­—å…¸ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    try:
        visualizer = DanmakuVisualizer()
        figures = visualizer.create_dashboard(analysis_result)
        return figures
        
    except Exception as e:
        st.error(f"âŒ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {str(e)}")
        return None


def analyze_danmaku_data(video_input, page_number, use_protobuf, date_filter, keyword_count, time_interval, enable_ai_analysis=False):
    """
    åˆ†æå¼¹å¹•æ•°æ®çš„ä¸»æ§åˆ¶å‡½æ•°
    
    Args:
        video_input: è§†é¢‘è¾“å…¥
        page_number: åˆ†På·
        use_protobuf: æ˜¯å¦ä½¿ç”¨protobufæ¥å£
        date_filter: æ—¥æœŸè¿‡æ»¤å™¨
        keyword_count: å…³é”®è¯æ•°é‡
        time_interval: æ—¶é—´é—´éš”
        enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æ
    """
    # åˆ›å»ºè¿›åº¦æ¡å’ŒçŠ¶æ€æ˜¾ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # è·å–UIæ˜¾ç¤ºé…ç½®
    ui_config = config.ui_display_settings

    try:
        # æ­¥éª¤1: è·å–å¼¹å¹•æ•°æ®
        status_text.text("ğŸ“¥ æ­£åœ¨è·å–å¼¹å¹•æ•°æ®...")
        progress_bar.progress(ui_config.get('progress_fetch_data', 20))
        
        danmaku_data = fetch_danmaku_data_step(video_input, page_number, use_protobuf, date_filter)
        if danmaku_data is None:
            return
            
        progress_bar.progress(ui_config.get('progress_analyze_data', 40))

        # æ­¥éª¤2: åˆ†ææ•°æ®
        status_text.text("ğŸ” æ­£åœ¨åˆ†æå¼¹å¹•æ•°æ®...")
        
        analysis_result = analyze_danmaku_step(danmaku_data, time_interval)
        if analysis_result is None:
            return
            
        progress_bar.progress(ui_config.get('progress_ai_analysis', 60))

        # æ­¥éª¤3: AIåˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if enable_ai_analysis:
            status_text.text("ğŸ¤– æ­£åœ¨è¿›è¡ŒAIæ™ºèƒ½åˆ†æ...")
            
        ai_results = perform_ai_analysis_step(analysis_result, danmaku_data, enable_ai_analysis)
        progress_bar.progress(ui_config.get('progress_visualization', 80))

        # æ­¥éª¤4: ç”Ÿæˆå¯è§†åŒ–
        status_text.text("ğŸ“Š æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        figures = generate_visualization_step(analysis_result)
        if figures is None:
            return

        progress_bar.progress(ui_config.get('progress_complete', 100))
        status_text.text("âœ… åˆ†æå®Œæˆ!")

        time.sleep(ui_config.get('progress_complete_delay', 1))
        progress_bar.empty()
        status_text.empty()

        # æ˜¾ç¤ºåˆ†æç»“æœ
        display_analysis_results(analysis_result, figures, danmaku_data, ai_results)

    except (ConnectionError, TimeoutError) as e:
        progress_bar.empty()
        status_text.empty()
        st.error("âŒ ç½‘ç»œè¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
        st.error(f"è¯¦ç»†ä¿¡æ¯: {str(e)}")
    except (ValueError, KeyError) as e:
        progress_bar.empty()
        status_text.empty()
        st.error("âŒ æ•°æ®å¤„ç†é”™è¯¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥æˆ–ç¨åé‡è¯•")
        st.error(f"è¯¦ç»†ä¿¡æ¯: {str(e)}")
    except MemoryError as e:
        progress_bar.empty()
        status_text.empty()
        st.error("âŒ å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•åˆ†ææ›´å°çš„è§†é¢‘æˆ–é‡å¯åº”ç”¨")
        st.error("å»ºè®®: é€‰æ‹©è¾ƒçŸ­çš„è§†é¢‘æˆ–å…³é—­å…¶ä»–åº”ç”¨é‡Šæ”¾å†…å­˜")
    except Exception as e:
        progress_bar.empty()
        status_text.empty()
        st.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        st.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        st.code(traceback.format_exc())


def display_analysis_results(analysis_result, figures, danmaku_data, ai_results=None):
    """æ˜¾ç¤ºåˆ†æç»“æœ"""

    st.markdown("---")
    st.header("ğŸ“Š åˆ†æç»“æœ")

    # åŸºæœ¬ç»Ÿè®¡
    if 'basic_stats' in analysis_result:
        st.subheader("ğŸ“ˆ åŸºæœ¬ç»Ÿè®¡")
        
        ui_config = config.ui_display_settings
        stats_columns_count = ui_config.get('stats_columns_count', 4)
        col1, col2, col3, col4 = st.columns(stats_columns_count)

        stats = analysis_result['basic_stats']

        with col1:
            st.metric("æ€»å¼¹å¹•æ•°", stats.get('total_count', 0))

        with col2:
            st.metric("ç‹¬ç‰¹å¼¹å¹•æ•°", stats.get('unique_count', 0))

        with col3:
            duplicate_rate = stats.get('duplicate_rate', 0)
            st.metric("é‡å¤ç‡", f"{duplicate_rate:.1%}")

        with col4:
            avg_length = analysis_result.get('length_stats', {}).get('mean_length', 0)
            st.metric("å¹³å‡é•¿åº¦", f"{avg_length:.1f}å­—")

    # AIåˆ†æç»“æœ
    if ai_results:
        st.markdown("---")
        st.subheader("ğŸ¤– AIæ™ºèƒ½åˆ†æ")
        
        # åˆ›å»ºAIåˆ†ææ ‡ç­¾é¡µ
        ai_tab1, ai_tab2, ai_tab3, ai_tab4 = st.tabs(["ğŸ˜Š AIæƒ…æ„Ÿåˆ†æ", "ğŸ¯ ä¸»é¢˜åˆ†æ", "ğŸ”¥ çƒ­ç‚¹è§£è¯»", "ğŸ“‹ ç»¼åˆæŠ¥å‘Š"])
        
        with ai_tab1:
            if 'sentiment_ai' in ai_results and 'analysis' in ai_results['sentiment_ai']:
                st.write("**AIæƒ…æ„Ÿåˆ†æç»“æœ:**")
                st.write(ai_results['sentiment_ai']['analysis'])
                if 'sample_count' in ai_results['sentiment_ai']:
                    st.caption(f"åŸºäº {ai_results['sentiment_ai']['sample_count']} æ¡å¼¹å¹•æ ·æœ¬åˆ†æ")
            elif 'sentiment_ai' in ai_results and 'error' in ai_results['sentiment_ai']:
                st.error(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {ai_results['sentiment_ai']['error']}")
            else:
                st.info("æš‚æ— AIæƒ…æ„Ÿåˆ†ææ•°æ®")
        
        with ai_tab2:
            if 'themes_ai' in ai_results and 'analysis' in ai_results['themes_ai']:
                st.write("**AIä¸»é¢˜åˆ†æç»“æœ:**")
                st.write(ai_results['themes_ai']['analysis'])
                if 'sample_count' in ai_results['themes_ai']:
                    st.caption(f"åŸºäº {ai_results['themes_ai']['sample_count']} æ¡å¼¹å¹•æ ·æœ¬åˆ†æ")
            elif 'themes_ai' in ai_results and 'error' in ai_results['themes_ai']:
                st.error(f"ä¸»é¢˜åˆ†æå¤±è´¥: {ai_results['themes_ai']['error']}")
            else:
                st.info("æš‚æ— AIä¸»é¢˜åˆ†ææ•°æ®")
        
        with ai_tab3:
            if 'hot_moments_ai' in ai_results and 'analysis' in ai_results['hot_moments_ai']:
                st.write("**AIçƒ­ç‚¹æ—¶åˆ»åˆ†æ:**")
                st.write(ai_results['hot_moments_ai']['analysis'])
                if 'hot_moments_count' in ai_results['hot_moments_ai']:
                    st.caption(f"åŸºäº {ai_results['hot_moments_ai']['hot_moments_count']} ä¸ªçƒ­ç‚¹æ—¶åˆ»åˆ†æ")
            elif 'hot_moments_ai' in ai_results and 'error' in ai_results['hot_moments_ai']:
                st.error(f"çƒ­ç‚¹åˆ†æå¤±è´¥: {ai_results['hot_moments_ai']['error']}")
            else:
                st.info("æš‚æ— AIçƒ­ç‚¹åˆ†ææ•°æ®")
        
        with ai_tab4:
            if 'comprehensive_ai' in ai_results and 'comprehensive_report' in ai_results['comprehensive_ai']:
                st.write("**AIç»¼åˆåˆ†ææŠ¥å‘Š:**")
                st.write(ai_results['comprehensive_ai']['comprehensive_report'])
                if 'sample_count' in ai_results['comprehensive_ai']:
                    st.caption(f"åŸºäº {ai_results['comprehensive_ai']['sample_count']} æ¡å¼¹å¹•æ ·æœ¬åˆ†æ")
            elif 'comprehensive_ai' in ai_results and 'error' in ai_results['comprehensive_ai']:
                st.error(f"ç»¼åˆåˆ†æå¤±è´¥: {ai_results['comprehensive_ai']['error']}")
            else:
                st.info("æš‚æ— AIç»¼åˆåˆ†ææ•°æ®")

    # å¯è§†åŒ–å›¾è¡¨
    if figures:
        st.markdown("---")
        st.subheader("ğŸ“Š æ•°æ®å¯è§†åŒ–")

        # åˆ›å»ºæ ‡ç­¾é¡µ
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["â˜ï¸ è¯äº‘å›¾", "ğŸ˜Š æƒ…æ„Ÿåˆ†æ", "ğŸ”¤ çƒ­é—¨å…³é”®è¯", "â° æ—¶é—´åˆ†å¸ƒ", "ğŸ“ é•¿åº¦åˆ†å¸ƒ", "ğŸ”¥ çƒ­ç‚¹æ—¶åˆ»"])

        with tab1:
            if 'wordcloud' in figures:
                st.plotly_chart(figures['wordcloud'], use_container_width=True)
            else:
                st.info("æš‚æ— è¯äº‘æ•°æ®")

        with tab2:
            if 'sentiment' in figures:
                st.plotly_chart(figures['sentiment'], use_container_width=True)
            else:
                st.info("æš‚æ— æƒ…æ„Ÿåˆ†ææ•°æ®")

        with tab3:
            if 'keywords' in figures:
                st.plotly_chart(figures['keywords'], use_container_width=True)
            else:
                st.info("æš‚æ— å…³é”®è¯æ•°æ®")

        with tab4:
            if 'time_distribution' in figures:
                st.plotly_chart(figures['time_distribution'], use_container_width=True)
            else:
                st.info("æš‚æ— æ—¶é—´åˆ†å¸ƒæ•°æ®")

        with tab5:
            if 'length_distribution' in figures:
                st.plotly_chart(figures['length_distribution'], use_container_width=True)
            else:
                st.info("æš‚æ— é•¿åº¦åˆ†å¸ƒæ•°æ®")

        with tab6:
            if 'hot_moments' in figures:
                st.plotly_chart(figures['hot_moments'], use_container_width=True)
            else:
                st.info("æš‚æ— çƒ­ç‚¹æ—¶åˆ»æ•°æ®")

    # è¯¦ç»†æ•°æ®
    with st.expander("ğŸ“‹ è¯¦ç»†æ•°æ®"):

        # çƒ­ç‚¹æ—¶åˆ»è¯¦æƒ…
        if 'hot_moments' in analysis_result and analysis_result['hot_moments']:
            st.subheader("ğŸ”¥ çƒ­ç‚¹æ—¶åˆ»è¯¦æƒ…")
            
            ui_config = config.ui_display_settings
            hot_moments_limit = ui_config.get('hot_moments_display_limit', 5)
            sample_limit = ui_config.get('sample_danmaku_display_limit', 3)
            detail_columns = ui_config.get('hot_moment_detail_columns', [1, 3])

            for i, moment in enumerate(analysis_result['hot_moments'][:hot_moments_limit], 1):
                with st.container():
                    col1, col2 = st.columns(detail_columns)

                    with col1:
                        st.write(f"**#{i}**")
                        st.write(f"æ—¶é—´: {format_duration(moment['time_start'])}")
                        st.write(f"å¼¹å¹•æ•°: {moment['count']}")

                    with col2:
                        st.write("**æ ·æœ¬å¼¹å¹•:**")
                        for j, text in enumerate(moment['sample_danmaku'][:sample_limit], 1):
                            st.write(f"{j}. {text}")

                    st.markdown("---")

        # åŸå§‹æ•°æ®ä¸‹è½½
        if danmaku_data:
            st.subheader("ğŸ’¾ æ•°æ®ä¸‹è½½")

            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(danmaku_data)
            
            ui_config = config.ui_display_settings
            preview_rows = ui_config.get('dataframe_preview_rows', 10)

            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            st.write("**æ•°æ®é¢„è§ˆ:**")
            st.dataframe(df.head(preview_rows), use_container_width=True)

            # ä¸‹è½½æŒ‰é’®
            csv = df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½å®Œæ•´å¼¹å¹•æ•°æ® (CSV)",
                data=csv,
                file_name=f"danmaku_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )


def format_duration(seconds):
    """
    æ ¼å¼åŒ–æ—¶é•¿æ˜¾ç¤º
    
    Args:
        seconds: ç§’æ•°
        
    Returns:
        str: æ ¼å¼åŒ–åçš„æ—¶é•¿å­—ç¬¦ä¸²
    """
    ui_config = config.ui_display_settings
    hour_threshold = ui_config.get('time_format_hour_threshold', 3600)
    minute_threshold = ui_config.get('time_format_minute_threshold', 60)
    
    if seconds < minute_threshold:
        return f"{seconds}ç§’"
    elif seconds < hour_threshold:
        minutes = seconds // minute_threshold
        secs = seconds % minute_threshold
        return f"{minutes}åˆ†{secs}ç§’"
    else:
        hours = seconds // hour_threshold
        minutes = (seconds % hour_threshold) // minute_threshold
        return f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ"


def format_number(num):
    """
    æ ¼å¼åŒ–æ•°å­—æ˜¾ç¤º
    
    Args:
        num: è¦æ ¼å¼åŒ–çš„æ•°å­—
        
    Returns:
        str: æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
    """
    ui_config = config.ui_display_settings
    billion_threshold = ui_config.get('number_format_billion_threshold', 100000000)
    wan_threshold = ui_config.get('number_format_wan_threshold', 10000)
    
    if num >= billion_threshold:  # 1äº¿
        return f"{num/billion_threshold:.1f}äº¿"
    elif num >= wan_threshold:  # 1ä¸‡
        return f"{num/wan_threshold:.1f}ä¸‡"
    else:
        return str(num)


if __name__ == "__main__":
    main()
